{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86b29663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "#%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "#from lab_utils_common import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic\n",
    "#from plt_quad_logistic import plt_quad_logistic, plt_prob\n",
    "#plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "\"\"\"\n",
    "lab_utils_common\n",
    "   contains common routines and variable definitions\n",
    "   used by all the labs in this week.\n",
    "   by contrast, specific, large plotting routines will be in separate files\n",
    "   and are generally imported into the week where they are used.\n",
    "   those files will import this file\n",
    "\"\"\"\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from ipywidgets import Output\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0')\n",
    "dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'\n",
    "dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\n",
    "#plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A scalar or numpy array of any size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "     g : array_like\n",
    "         sigmoid(z)\n",
    "    \"\"\"\n",
    "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
    "    g = 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "    return g\n",
    "\n",
    "##########################################################\n",
    "# Regression Routines\n",
    "##########################################################\n",
    "\n",
    "def predict_logistic(X, w, b):\n",
    "    \"\"\" performs prediction \"\"\"\n",
    "    return sigmoid(X @ w + b)\n",
    "\n",
    "def predict_linear(X, w, b):\n",
    "    \"\"\" performs prediction \"\"\"\n",
    "    return X @ w + b\n",
    "\n",
    "def compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):\n",
    "    \"\"\"\n",
    "    Computes cost using logistic loss, non-matrix version\n",
    "\n",
    "    Args:\n",
    "      X (ndarray): Shape (m,n)  matrix of examples with n features\n",
    "      y (ndarray): Shape (m,)   target values\n",
    "      w (ndarray): Shape (n,)   parameters for prediction\n",
    "      b (scalar):               parameter  for prediction\n",
    "      lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization\n",
    "      safe : (boolean)          True-selects under/overflow safe algorithm\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m,n = X.shape\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()\n",
    "        if safe:  #avoids overflows\n",
    "            cost += -(y[i] * z_i ) + log_1pexp(z_i)\n",
    "        else:\n",
    "            f_wb_i = sigmoid(z_i)                                                   #(n,)\n",
    "            cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar\n",
    "    cost = cost/m\n",
    "\n",
    "    reg_cost = 0\n",
    "    if lambda_ != 0:\n",
    "        for j in range(n):\n",
    "            reg_cost += (w[j]**2)                                               # scalar\n",
    "        reg_cost = (lambda_/(2*m))*reg_cost\n",
    "\n",
    "    return cost + reg_cost\n",
    "\n",
    "\n",
    "def log_1pexp(x, maximum=20):\n",
    "    ''' approximate log(1+exp^x)\n",
    "        https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice\n",
    "    Args:\n",
    "    x   : (ndarray Shape (n,1) or (n,)  input\n",
    "    out : (ndarray Shape matches x      output ~= np.log(1+exp(x))\n",
    "    '''\n",
    "\n",
    "    out  = np.zeros_like(x,dtype=float)\n",
    "    i    = x <= maximum\n",
    "    ni   = np.logical_not(i)\n",
    "\n",
    "    out[i]  = np.log(1 + np.exp(x[i]))\n",
    "    out[ni] = x[ni]\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_cost_matrix(X, y, w, b, logistic=False, lambda_=0, safe=True):\n",
    "    \"\"\"\n",
    "    Computes the cost using  using matrices\n",
    "    Args:\n",
    "      X : (ndarray, Shape (m,n))          matrix of examples\n",
    "      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      w : (ndarray  Shape (n,) or (n,1))  Values of parameter(s) of the model\n",
    "      b : (scalar )                       Values of parameter of the model\n",
    "      verbose : (Boolean) If true, print out intermediate value f_wb\n",
    "    Returns:\n",
    "      total_cost: (scalar)                cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y = y.reshape(-1,1)             # ensure 2D\n",
    "    w = w.reshape(-1,1)             # ensure 2D\n",
    "    if logistic:\n",
    "        if safe:  #safe from overflow\n",
    "            z = X @ w + b                                                           #(m,n)(n,1)=(m,1)\n",
    "            cost = -(y * z) + log_1pexp(z)\n",
    "            cost = np.sum(cost)/m                                                   # (scalar)\n",
    "        else:\n",
    "            f    = sigmoid(X @ w + b)                                               # (m,n)(n,1) = (m,1)\n",
    "            cost = (1/m)*(np.dot(-y.T, np.log(f)) - np.dot((1-y).T, np.log(1-f)))   # (1,m)(m,1) = (1,1)\n",
    "            cost = cost[0,0]                                                        # scalar\n",
    "    else:\n",
    "        f    = X @ w + b                                                        # (m,n)(n,1) = (m,1)\n",
    "        cost = (1/(2*m)) * np.sum((f - y)**2)                                   # scalar\n",
    "\n",
    "    reg_cost = (lambda_/(2*m)) * np.sum(w**2)                                   # scalar\n",
    "\n",
    "    total_cost = cost + reg_cost                                                # scalar\n",
    "\n",
    "    return total_cost                                                           # scalar\n",
    "\n",
    "def compute_gradient_matrix(X, y, w, b, logistic=False, lambda_=0):\n",
    "    \"\"\"\n",
    "    Computes the gradient using matrices\n",
    "\n",
    "    Args:\n",
    "      X : (ndarray, Shape (m,n))          matrix of examples\n",
    "      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      w : (ndarray  Shape (n,) or (n,1))  Values of parameters of the model\n",
    "      b : (scalar )                       Values of parameter of the model\n",
    "      logistic: (boolean)                 linear if false, logistic if true\n",
    "      lambda_:  (float)                   applies regularization if non-zero\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1))     The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db: (scalar)                     The gradient of the cost w.r.t. the parameter b\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y = y.reshape(-1,1)             # ensure 2D\n",
    "    w = w.reshape(-1,1)             # ensure 2D\n",
    "\n",
    "    f_wb  = sigmoid( X @ w + b ) if logistic else  X @ w + b      # (m,n)(n,1) = (m,1)\n",
    "    err   = f_wb - y                                              # (m,1)\n",
    "    dj_dw = (1/m) * (X.T @ err)                                   # (n,m)(m,1) = (n,1)\n",
    "    dj_db = (1/m) * np.sum(err)                                   # scalar\n",
    "\n",
    "    dj_dw += (lambda_/m) * w        # regularize                  # (n,1)\n",
    "\n",
    "    return dj_db, dj_dw                                           # scalar, (n,1)\n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters, logistic=False, lambda_=0, verbose=True):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      X (ndarray):    Shape (m,n)         matrix of examples\n",
    "      y (ndarray):    Shape (m,) or (m,1) target value of each example\n",
    "      w_in (ndarray): Shape (n,) or (n,1) Initial values of parameters of the model\n",
    "      b_in (scalar):                      Initial value of parameter of the model\n",
    "      logistic: (boolean)                 linear if false, logistic if true\n",
    "      lambda_:  (float)                   applies regularization if non-zero\n",
    "      alpha (float):                      Learning rate\n",
    "      num_iters (int):                    number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray): Shape (n,) or (n,1)    Updated values of parameters; matches incoming shape\n",
    "      b (scalar):                         Updated value of parameter\n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    w = w.reshape(-1,1)      #prep for matrix operations\n",
    "    y = y.reshape(-1,1)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = compute_gradient_matrix(X, y, w, b, logistic, lambda_)\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion\n",
    "            J_history.append( compute_cost_matrix(X, y, w, b, logistic, lambda_) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            if verbose: print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "\n",
    "    return w.reshape(w_in.shape), b, J_history  #return final w,b and J history for graphing\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "\n",
    "    Args:\n",
    "      X (ndarray): Shape (m,n) input data, m examples, n features\n",
    "\n",
    "    Returns:\n",
    "      X_norm (ndarray): Shape (m,n)  input normalized by column\n",
    "      mu (ndarray):     Shape (n,)   mean of each feature\n",
    "      sigma (ndarray):  Shape (n,)   standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma\n",
    "\n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "#check our work\n",
    "#from sklearn.preprocessing import scale\n",
    "#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "\n",
    "######################################################\n",
    "# Common Plotting Routines\n",
    "######################################################\n",
    "\n",
    "\n",
    "def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):\n",
    "    \"\"\" plots logistic data with two axis \"\"\"\n",
    "    # Find Indices of Positive and Negative Examples\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n",
    "    neg = neg.reshape(-1,)\n",
    "\n",
    "    # Plot examples\n",
    "    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)\n",
    "    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)\n",
    "    ax.legend(loc=loc)\n",
    "\n",
    "    ax.figure.canvas.toolbar_visible = False\n",
    "    ax.figure.canvas.header_visible = False\n",
    "    ax.figure.canvas.footer_visible = False\n",
    "\n",
    "def plt_tumor_data(x, y, ax):\n",
    "    \"\"\" plots tumor data on one axis \"\"\"\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "\n",
    "    ax.scatter(x[pos], y[pos], marker='x', s=80, c = 'red', label=\"malignant\")\n",
    "    ax.scatter(x[neg], y[neg], marker='o', s=100, label=\"benign\", facecolors='none', edgecolors=dlblue,lw=3)\n",
    "    ax.set_ylim(-0.175,1.1)\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlabel('Tumor Size')\n",
    "    ax.set_title(\"Logistic Regression on Categorical Data\")\n",
    "\n",
    "    ax.figure.canvas.toolbar_visible = False\n",
    "    ax.figure.canvas.header_visible = False\n",
    "    ax.figure.canvas.footer_visible = False\n",
    "\n",
    "# Draws a threshold at 0.5\n",
    "def draw_vthresh(ax,x):\n",
    "    \"\"\" draws a threshold \"\"\"\n",
    "    ylim = ax.get_ylim()\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.fill_between([xlim[0], x], [ylim[1], ylim[1]], alpha=0.2, color=dlblue)\n",
    "    ax.fill_between([x, xlim[1]], [ylim[1], ylim[1]], alpha=0.2, color=dldarkred)\n",
    "    ax.annotate(\"z >= 0\", xy= [x,0.5], xycoords='data',\n",
    "                xytext=[30,5],textcoords='offset points')\n",
    "    d = FancyArrowPatch(\n",
    "        posA=(x, 0.5), posB=(x+3, 0.5), color=dldarkred,\n",
    "        arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n",
    "    )\n",
    "    ax.add_artist(d)\n",
    "    ax.annotate(\"z < 0\", xy= [x,0.5], xycoords='data',\n",
    "                 xytext=[-50,5],textcoords='offset points', ha='left')\n",
    "    f = FancyArrowPatch(\n",
    "        posA=(x, 0.5), posB=(x-3, 0.5), color=dlblue,\n",
    "        arrowstyle='simple, head_width=5, head_length=10, tail_width=0.0',\n",
    "    )\n",
    "    ax.add_artist(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4f9205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32589e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    y_hat = X @ w + b\n",
    "    error = y_hat - y\n",
    "    dj_dw = (1/m) * (X.T @ error)\n",
    "    dj_db = (1/m) * np.sum(error)\n",
    "    return dj_db, dj_dw\n",
    "\n",
    "def compute_cost_linear(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    y_hat = X @ w + b\n",
    "    cost = (1/(2*m)) * np.sum((y_hat - y)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71d30b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.8191234538503088e-08),\n",
       " array([ 1.2 , -3.45, -0.52, -1.04,  0.3 ,  3.82]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "with open(\"./seven_game_series_diffs_win_lose.csv\", 'r') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    for row in csvreader:\n",
    "        x_train.append(row)\n",
    "x_train.pop(0)\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = [int(item) for item in x_train[i]]\n",
    "    y_train.append(x_train[i][-1])\n",
    "    x_train[i].pop()\n",
    "    #x_train[i] = np.asarray(x_train[i])\n",
    "    #y_train[i] = np.asarray(y_train[i])\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "#w = np.full(len(x_train[0]), 1)\n",
    "w = np.random.randn(6)*10\n",
    "b = 0.5\n",
    "\n",
    "compute_gradient_logistic(x_train, y_train, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06117930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.96,  -3.79, -21.38,  -7.23,  -2.91,  -3.07])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.randn(6)*10\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e1ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usolon\\AppData\\Local\\Temp\\ipykernel_31792\\3119260743.py:85: RuntimeWarning: divide by zero encountered in log\n",
      "  cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar\n",
      "C:\\Users\\usolon\\AppData\\Local\\Temp\\ipykernel_31792\\3119260743.py:85: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost nan   \n",
      "Iteration   50: Cost 0.6340776827576907   \n",
      "Iteration  100: Cost 0.6290809502471841   \n",
      "Iteration  150: Cost 0.6290809491615617   \n",
      "Iteration  200: Cost 0.629080949161561   \n",
      "Iteration  250: Cost 0.6290809491615611   \n",
      "Iteration  300: Cost 0.6290809491615612   \n",
      "Iteration  350: Cost 0.6290809491615611   \n",
      "Iteration  400: Cost 0.6290809491615611   \n",
      "Iteration  450: Cost 0.6290809491615611   \n",
      "\n",
      "updated parameters: w:[ 0.03  0.02  0.    0.    0.03 -0.01], b:-2.4288214342767663e-16\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "#w_tmp = np.full(len(x_train[0]), 1)\n",
    "w_tmp = np.random.randn(6)\n",
    "b_tmp = 0 ##50 -50 chance to begin with\n",
    "alph = 0.01\n",
    "iters = 500\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(x_train, y_train, w_tmp, b_tmp, alph, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72bb2186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9999998872615166)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.asarray([ 0.25,  0.22, -0.14, -0.22,  0.36, -0.35])\n",
    "b= 0.49819489463253525\n",
    "x1 = np.asarray([123,111,128,99, 97, 103])\n",
    "x2 = np.asarray([116,105,102,116, 110, 104])\n",
    "sigmoid(np.dot(x2,w)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6407c186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03  0.02  0.    0.    0.03 -0.01]\n",
      "-2.4288214342767663e-16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.497511810804329)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.asarray([7, 6, 26, -17, -13,-1])\n",
    "print(w_out)\n",
    "print(b_out)\n",
    "sigmoid(np.dot(x,w_out)+b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca0c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e4409f3",
   "metadata": {},
   "source": [
    "OKC vs IND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9c3f23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03  0.02  0.    0.    0.03 -0.01]\n",
      "-2.4288214342767663e-16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6946573029626172)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.asarray([-1, 16, -9, 7, 11,-17])\n",
    "print(w_out)\n",
    "print(b_out)\n",
    "sigmoid(np.dot(x,w_out)+b_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
